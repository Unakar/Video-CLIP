# 第一周任务

For everyone:

1. 阅读并运行 https://github.com/openai/CLIP 里README.md中的代码，了解CLIP的基本使用方法
2. 在colab里运行https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb，（该notebook也已放在Tutorials文件夹下，可以本地运行测试）
3. 在survey文件夹下开始进行某个任务的调研报告(找github上现成的clip应用)

closed: 2024/4/6 

## 第一周会议记录：谢天

清明期间组织了2h30min的小组会议。会议内容是对CLIP在video retrieval,video recognition,prompt learning等方面的应用进行了讨论。

大家表现得都十分出色，每人在过去一周都读了1-2篇论文，讨论得很激烈。

1. 洪毓谦做了clip4clip的论文解读。这篇文章在CLIP一经发表后一个半月就出来了，想法比较朴素，好处是易于实现。对我们最大的启示就是mean pooling的方法其实在现实场景里很有优越性：基于transformer的后处理层甚至text-image-time三者预先融合的方法看似引入了很多可学习的参数用来进行时序建模，其实受限于数据集质量很多。于是我敲定，我们的工作里对视频流处理就是简单的多帧取均值，融合得到特征向量。

2. 陈鹤影讲解了一篇CVPR2023 重新审视clip在时空建模下的应用。讲的很好。主要是介绍了STAN时空网络，相比上一篇clip4clip有了很大的提升。但是受限于我们服务器的条件(8卡3090，且只有128的磁盘，连数据集都装不下)，故而我选择放弃STAN。但是此篇证明了类似大模型中adapter的方式进行旁路建模，效果比中间插层/最后修改输出层效果更好，（我觉得训练也更高效，推理速度也快）

3. 林洛昊也是一篇CVPR2023的工作，Vita-Clip.这篇就很specific domain，就是视频理解的领域，
它把视频图像处理人为先验地从三个层面提取特征：当前帧，邻近帧，全局帧做summary三者融合到一起，在消融实验中发现其实它的方法提升并不显著(<2%),emmm，费这么大力提升这么少，我觉得不值得。另外，它在文本tokenzier前引入了可学习的prompt，在prompt前加入了类别标签，把原本迁移性能很好的clip硬是限定死了在某些类别上，效果当然变好了，但我觉得这是在开历史倒车，水论文而已

4. 张又升同学讲了一篇ACL的工作，AltClip。前面的工作都是改进image encoder，这篇把text encoder换成双语编码器，利用最简单的知识蒸馏去让XLMR与CLIP text encoder做一个接近，然后fc层映射到相同维度做对齐....就很水，而且蒸馏技术非常一般，特征融合也没什么好说的。最后效果相比CLIP也没有多少提升，这个场景也看不出意义(不如调用谷歌翻译api把clip输出的英语翻译为任意语言，重训模型是费力且效果不可控的)

## 工作推进总结

视频检索的pipeline已经搭好了。

模型使用clip4clip的mean pooling方法，视频数据集已经过encoder处理为embedding放入向量数据库做存储查询.

检索使用cvpr2022的querybank算法对cosine similarity计算做修正，另外使用二分查找加速，查询5个视频时间在1s以内。

我们在MSRVTT数据集上对原始模型进行了微调，微调后效果如下，略有提升

Evaluation：
| MODEL | R@1 | R@5 | R@10 | Median R | Mean R |
| --- | --- | --- | --- | --- | --- |
| origin model | 37.16 | 62.10	| 71.16	| 3.0 |	3.0	| 42.2 |
| msr-vtt finetuned | 38.38 | 62.89 | 72.01 | 3.0 |	3.0	| 39.3 |

在对搜索算法做修正改进后，R1召回率上升至45.75,性能获得巨大提升